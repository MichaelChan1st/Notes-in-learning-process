## AlexNet

* 在此之前，深度学习主要是以无监督的形式训练，因为有监督学习根本打不赢无监督的机器学习。讨论了深度神经网络的性能和网络的深度有关
* 使用了CNN来进行分类，使用trick解决了CNN过拟合不容易训练的问题
* 第一次用了EndtoEnd的方式训练（对输入数据不进行任何预处理，直接放入网络得到结果），但是作者没有凸显这一重大贡献 

## Vision Transformer

==在*大*数据集上做预训练==，为什么需要大数据集上与训练呢？作者也给出了他的猜想：transformer不像CNN具有两个归纳偏置（位置不变性和平移不变性）。尽可能与Transformer相同的架构，将输入图片切成P（9）个patches，其中每一个patch由$16\times 16\times 3$个像素组成，这样就能降低模型的输入大小，像NLP一样，有P个输入，每个输入有$16\times16=192$维经过一个embedding层的转换就成了NLP中一样的模式，加上一个class类别token用于接收最后特征。

在一些大图片上识别任务上，依然是CNN效果好。如果图片太大，那么输入序列也会相应变大。

由于在预训练中学得的位置编码是确定的，所以在微调中，不太方面更改输入尺寸，虽然可以使用差值操作，但可能会掉点。

开启了Transformer在视觉领域的新纪元

这篇工作还是留下了遗憾：未能像NLP领域一样使用无监督训练Transformer 

> 后续对ViT的补充

* ViT-FRCNN——将ViT用在了目标检测领域
* SETR——将ViT用到了分割领域

> 其他相关神作

* MAE——在分割任务上，让生成模型的效果超过判别模型，进行self- supervised进行预训练，在ImageNet1K上就可以训练的很好。类似于BERT完形填空方式进行无监督学习
* DETR——目标检测任务上的力作，改变了出框的方式
* Noisy Student——使用传统的CNN方式将分类任务做的最好的一个模型，使用伪标签pseudo-label

## Masked Autoencoders Are Scalable Vision Learners

MAE

采用自监督的方式进行预训练，得到图片的特征。在输入上给图片随机mask，将未mask部分放入encoder中计算，得到embedding结果后按照一定方法补齐mask部分embedding，最后放入decoder中计算得到的结果与原图计算损失。

encoder使用Vision Transformer的架构，decoder是另外一个transformer，在ImageNet-1K上做与训练，只需要加入合适的正则项就可以训练ViT

1. 遮住更多的块，使得预训练任务更加难，模型更加鲁棒
2. 使用Transformer作为解码器，直接还原原始像素信息，使得整个流程更加简单一点
3. 加上ViT工作之后的各种Tricks

## Momentum Contrast for Unsupervised Visual Representation Learning-MoCo

是cvpr2020的最佳论文提名

视觉领域里使用对比学习的一个里程碑式工作

