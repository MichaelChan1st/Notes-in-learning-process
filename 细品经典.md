# AlexNet

> 深度学习的奠基作之一



* 创新性引入三大方面 Three Dirty Tricks
  1. 图片增强
  2. ReLU
  3. Dropout
* 一大贡献
  * 证明了深度神经网络训练出来的图片的最后的那个向量，在语义空间的表示非常好：作者使用某个图片最后的向量，寻找所有图片中最后的向量与之相近的，得到发现这些图片都是非常相似的（或者同一类型）
* 二大贡献
  * 提出了一个端到端模型：从图片进入神经网络到神经网络输出预期结果，没有进行额外的例如特征提取的工作

---

* 自己的不足
  * SGD算法：
    * BGD每次走的方向是original-loss的负梯度方向，是original-loss在当前点上的最速下降方向。而SGD每次走的方向是minibatch-loss的负梯度方向（或者理解成original-loss的负梯度+randomness），显然这个方向和original-loss的负梯度方向不同，也就不是original-loss在当前位置的最快下降方向（如果这个mini batch的大部分数据点的target是错误的，甚至有可能是original-loss在当前位置的上升方向），所以使用SGD算法从当前点走到global minimal的路径会很曲折（震荡）
    * https://www.bilibili.com/video/BV1JE411g7XF?p=5
    * https://zhuanlan.zhihu.com/p/152566066
  * 学习率调整：
    * https://zhuanlan.zhihu.com/p/344294796
  * BatchNorm：
    * https://zhuanlan.zhihu.com/p/54073204?ivk_sa=1024320u
    * （论batchnorm作用）https://arxiv.org/abs/1805.11604

------

# ResNet

* COCO数据集用于目标检测
* ResNet有一个理论是100层能实现的效果，1000层一定可以，大不了让多余的层不进行任何转换。

* 假如残差连接之后使得网络参数更加稳定，收敛的更快

> 假设在原模型$g(x)$的基础上加深了几层使得模型输出为：$f(g(x))$现在对模型求梯度：$\frac{\nabla f(g(x))}{\nabla x} = \frac{\nabla f(g(x))}{\nabla g(x)}\times \frac{\nabla g(x)}{\nabla x}$会经过累乘导致梯度消失。
>
> 但是如果是通过残差连接加深层数的话，模型输出为：$f(g(x))+g(x)$,模型梯度为：
>
> $\frac{\nabla (f(g(x))+g(x))}{\nabla x}=\frac{\nabla f(g(x))}{\nabla g(x)}\times \frac{\nabla g(x)}{\nabla x}+\frac{\nabla g(x)}{\nabla x}$,对于次，如果梯度消失，任然有未加深之前的梯度$\frac{\nabla g(x)}{\nabla x}$