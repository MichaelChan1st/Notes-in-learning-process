

# 深度学习中的知识点积累可用于面试

* `Focal loss`能够解决多类数据不平衡问题

* 图像数据的标准化和BatchNorm的区别：

  * 数据标准化是把图像的值（三通道图像里每个像素点上每个通道都有相应的值，这个值的范围通常是0-255），转到 (-1, 1)，针对的是单个图像而且==目的只是便于网络计算==

    Batch Normalization是对一个Batch的数据，==让它们数据的分布更加均衡==，比如几个图片是白天几个图片是晚上，图像值的分布偏差较大，加了BN会使得数据分布偏差较小，==网络能更好的学到目标的特征而不是学会了区分白天目标主要出现在什么位置、晚上目标主要出现在什么位置==。

  > [BatchNorm的作用](https://www.cnblogs.com/hoojjack/p/12350707.html)（随着研究深入，说法可能有变）

  * 机器学习领域有个很重要的假设：**IID独立同分布假设**，就是假设训练数据和测试数据是满足相同分布的，这是通过训练数据获得的模型能够在测试集获得好的效果的一个基本保障。那BatchNorm的作用是什么呢？**BatchNorm就是在深度神经网络训练过程中使得每一层神经网络的输入保持相同分布的。**

  * 接着引入**covariate shift的概念**：**如果ML系统实例集合<X,Y>中的输入值X的分布老是变，这不符合IID假设**，网络模型很难**稳定的学规律**，这不得引入迁移学习才能搞定吗，我们的ML系统还得去学习怎么迎合这种分布变化啊。对于深度学习这种包含很多隐层的网络结构，在训练过程中，因为各层参数不停在变化，所以每个隐层都会面临covariate shift的问题，也就是**在训练过程中，隐层的输入分布老是变来变去，这就是所谓的“Internal Covariate Shift”，Internal指的是深层网络的隐层，是发生在网络内部的事情，而不是covariate shift问题只发生在输入层。**

  * BatchNorm的来历：

    ​	BN不是凭空拍脑袋拍出来的好点子，它是有启发来源的：之前的研究表明如果在图像处理中对输入图像进行白化（Whiten）操作的话——所谓**白化**，**就是对输入数据分布变换到0均值，单位方差的正态分布**——那么神经网络会较快收敛，那么BN作者就开始推论了：图像是深度神经网络的输入层，做白化能加快收敛，那么其实对于深度网络来说，其中某个隐层的神经元是下一层的输入，意思是其实深度神经网络的每一个隐层都是输入层，不过是相对下一层来说而已，那么能不能对每个隐层都做白化呢？这就是启发BN产生的原初想法，而BN也确实就是这么做的，**可以理解为对深层神经网络每个隐层神经元的激活值做简化版本的白化操作。**

  * BatchNorm的本质思想：

    ​	BN的基本思想其实相当直观：因为深层神经网络在做非线性变换前的**激活输入值**（就是那个x=WU+B，U是输入）**随着网络深度加深或者在训练过程中，其分布逐渐发生偏移或者变动，之所以训练收敛慢，一般是整体分布逐渐往非线性函数的取值区间的上下限两端靠近**（对于Sigmoid函数来说，意味着激活输入值WU+B是大的负值或正值），所以这**导致反向传播时低层神经网络的梯度消失**，这是训练深层神经网络收敛越来越慢的**本质原因**，**而BN就是通过一定的规范化手段，把每层神经网络任意神经元这个输入值的分布强行拉回到均值为0方差为1的标准正态分布**，其实就是把越来越偏的分布强制拉回比较标准的分布，这样使得激活输入值落在非线性函数对输入比较敏感的区域，这样输入的小变化就会导致损失函数较大的变化，意思是**这样让梯度变大，避免梯度消失问题产生，而且梯度变大意味着学习收敛速度快，能大大加快训练速度。**	

  BatchNorm2d数学公式：BatchNorm2d(num_features,eps,momentum,affine)
  $$
  y=\frac{x-\bar x}{\sqrt{S(x)}+\epsilon}\times \gamma + \beta
  $$

  1. num_features：一般输入参数为通道数
  2. 分母中添加的一个值，目的是为了计算的稳定性，默认为：1e-5
  3. 一个用于运行过程中均值和方差的一个估计参数
  4. 当设为true时，会给定可以学习的系数矩阵gamma和beta

----

$$
\begin{aligned}
y=&Sigmoid(w_1\times x+b_1)\\
z=&Sigmoid(w_2\times y+b_2)\\
t=&w_3\times z+b_3
\end{aligned}
$$

求导过程如下：
$$
\begin{aligned}
\frac{\partial t}{\partial w_1} &=\frac{\partial t}{\partial z}
\times \frac{\partial z}{\partial y}\times \frac{\partial y}{\partial w_1} \\
&=w_3\times \frac{\partial z}{Sigmoid(w_2\times y+b_2)}\times w_2\times  \frac{\partial y}{Sigmoid(w_1\times y+b_1)}\times x
\end{aligned}
$$

---

卷积公式：
$$
outsize=\frac{inputsize-ksize+2\times padding}{stride}+1
$$


```python
a = [np.array([[[1],
                [2],
                [3]],
               [[4],
                [5],
                [6]]]),
     np.array([[[7],
                
                [8],
                [9]],
            [[10],
                [11],
                [12]]])]
> a[0].shape,np.vstack(a).shape
[out]:(2, 3, 1), (4, 3, 1)
> a[0].shape,np.hstack(a).shape
[out]:(2, 3, 1), (2, 6, 1)
结论：np.hstack和np.vstack只变动1，2维数据
```

