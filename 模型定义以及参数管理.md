# 模型定义以及参数管理

## torch中的nn.Module类

### 模拟Sequential类的实现

```python
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        for block in args:
            # self._modules是继承下来的方法名，不能够更改
            self._modules[block] = block
    def forward(self, x):
        for block in self._modules.values():
            x = block(x)
        return x
```

* 其中的_modules是父类的属性
* 我们可以创建一个字典用于存储layer

```python
class MySequential(nn.Module):
    def __init__(self, *args):
        super().__init__()
        self.modules = OrderedDict()
        for block in args:
            self.modules[block] = block
    def forward(self, x):
        for _,block in self.modules.items():
            x = block(x)
        return x
```

```python
net = MySequential(nn.Linear(20,256), nn.ReLU(), nn.Linear(256, 125))
x = torch.rand((10,20))
net(x)
```



----

### 对Sequential的扩展

* 为了解决Sequential只能顺序执行的不足

```python
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.rand_weight = torch.rand((20,20), requires_grad=False)
        self.linear = nn.Linear(20,20)
    def forward(self, x):
        x = self.linear(x)
        x = F.relu(torch.mm(x, self.rand_weight) + 1)
        x = self.linear(x)
        while x.abs().sum() > 1:
            x /= 2
        return x.sum()
```

* 可以混合搭配各种组合快的方法

```python
class NestMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(20,64),nn.ReLU(),
                                nn.Linear(64, 32),nn.ReLU())
        self.linear = nn.Linear(32, 16)
    def forward(self, x):
        return self.linear(self.net(x))
X = torch.rand((10,20))
chimera = nn.Sequential(NestMLP(), nn.Linear(16, 20), FixedHiddenMLP())
chimera(X)
```

---

## 参数管理

### 参数访问

```python
net = nn.Sequential(nn.Linear(20,5), nn.ReLU(), nn.Linear(5, 5))
print(net[2].state_dict())
"""
注意这里如果使用自己构建的MySequential对象是不支持下表访问的
net[2]指的的nn.Linear层
net[2].state_dict():状态OrderedDict字典，存储weight和bias参数
"""
```

[状态OrderedDict字典]:按照有序插入顺序存储的有序字典

> output 

```python
in: net[2]
output: Linear(in_features=5, out_features=5, bias=True)
in: net[2].state_dict()
output: OrderedDict([('weight', tensor([[ 0.3353,  0.2070, -0.4416, -0.1352, -0.0927],
        [-0.2659,  0.0772,  0.3812, -0.4449, -0.1303],
        [-0.2290,  0.3134, -0.3976,  0.2084, -0.3162],
        [ 0.2307, -0.0344, -0.1869,  0.4254, -0.2971],
        [ 0.3984,  0.1386,  0.2486, -0.3742,  0.0153]])), ('bias', tensor([-0.2279,  0.1941, -0.4337,  0.0959,  0.2128]))])
```

> 参数的结构net.state_dict()

```python
# 1.创建
from collection import OrderedDict
dic = OrderedDict()
dic['0.weight']=tensor([……])
dic['0.bias']=tensor([……])
dic['2.weight']=tensor([……])
dic['2.bias']=tensor([……])
# 2.形状
OrderedDict(
  [('0.weight',tensor()),
  ('0.bias',tensor()),
  ('2.weight',tensor()),
  ('bias',tensor())]
)
```

> 参数访问的另一种方式

```net[i].weight | net[i].weight.data | net[i].weight.grad```

```python
type(net[2].bias)
# output:torch.nn.parameter.Parameter
net[2].bias
# output:Parameter containing:tensor([-0.2279,  0.1941, -0.4337,  0.0959,  0.2128], requires_grad=True)
net[2].bias.data
# tensor([-0.2279,  0.1941, -0.4337,  0.0959,  0.2128])
```

> 访问指定层参数的梯度

```python
net[2].weight.grad
```

### 嵌套层的参数访问

1. 首先要会查看网络结构

   ```python
   print(net)
   
   # output:
   1:Sequential(
   2:  (0): Sequential(
   3:    (0): Linear(in_features=20, out_features=8, bias=True)
   4:    (1): ReLU()
   5:    (2): Linear(in_features=8, out_features=20, bias=True)
   6:    (3): ReLU()
   7:  )
   8:  (1): Linear(in_features=20, out_features=1, bias=True)
   10:)
   ```

2. 分析结构组成

   ```python
   net--> 指的是最外层的Sequential
   net[0]--> 指的是第二行：(0): Sequential()
   net[0][0]--> 指的是第三行：(0): Linear()
   # 在net[0][0]或者net[1]可以进行参数访问
   net[0][0].weight
   ```



### 参数初始化

* 如何对一个大的网络进行初始化？

```python
def init_normal(m):
    if type(m) == nn.Linear:
        nn.init.normal_(m.weight, mean=0, std=0.01)
        nn.init.zeros_(m.bias)
        
net.apply(init_normal)
```

* 如何对局部的网络进行不同的初始化？

```python
def xavier(m):
  if type(m) == nn.Linear:
    nn.init.xavier_uniform_(m.weight)
def init_42(m):
  if type(m) == nn.Linear:
    nn.init.constant_(m.weight, 42)
net[0].apply(xavier)
net[2].apply(init_42)
```

* 如何自定义初始化

  * `net.named_parameters()`会返回一个生成器，其作用是返回网络中所有参数的名称和数据

    * `net[0].named_parameters()`

      * 返回`net[0]这一层的所有参数`

    * `net.named_parameters()`

      * 返回`net`整个网络中的所有参数但是名字中会带有序号

      * ```python
        [('0.weight', torch.Size([5, 20])),
         ('0.bias', torch.Size([5])),
         ('2.weight', torch.Size([5, 5])),
         ('2.bias', torch.Size([5]))]
        ```

```python
def my_init(m):
    if type(m) == nn.Linear:
        print(
            "init",
            *[(name, param.shape) for name, param in m.named_parameters()][0])
        nn.init.uniform_(m.weight.data, -10, 10)
        # 仅保留weight.data中绝对值大于5的值，其他的置为零
        m.weight.data *= m.weight.data.abs() >= 5
net.apply(my_init)
```

### 参数的修改

```python
net[0].weight.data[0,0] = 42
net[0].weight.data += 1
# 会直接替换weight.data里面的值
```

### 参数的应用

* 共享参数

  * 使得网络不管如何更新，都不会造成共享层之间参数的不同

  ```python
  # 先单独拎出来需要共享的层，给它单独声明
  shared = nn.Linear(8, 8)
  
  net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), shared, nn.ReLU(), shared, nn.ReLU(), nn.Linear(8, 1))
  
  net1[2].weight.data == net[4].weight.data
  
  # 如果更改共享层中一层的参数，那么另一层也会跟着改变
  net[2].weight.data[0,0] = 100
  net[2].weight.data[0,0] == net[4].weight.data[0,0]
  ```

* 创建参数

  * `nn.Parameter()`默认计算参数的梯度，能够给参数一个合适的名字
  * 自定义一个带参数图层

  ```python
  class MyLinear(nn.Module):
      def __init__(self, in_units, units):
          super().__init__()
          self.weight = nn.Parameter(torch.randn(in_units, units))
          self.bias = nn.Parameter(torch.randn(units,))
      def forward(self, x):
          linear = torch.matmul(x, self.weight.data) + self.bias.data
          return F.relu(linear)
  dense = MyLinear(5,3)
  dense.weight
  ```

  ### 参数的保存

  `torch.save(net.state_dict(), 'mlp.params')`

  `torch.load('mlp.params')`

  `torch.load_state_dict(torch.load(''))`

----

# optimizer.param_groups

> optimizer.param_groups： 是长度为2的list，其中的元素是2个字典；
>
> optimizer.param_groups[0]： 长度为6的字典，包括[‘amsgrad’, ‘params’, ‘lr’, ‘betas’, ‘weight_decay’, ‘eps’]这6个参数；
>
> optimizer.param_groups[1]： 好像是表示优化器的状态的一个字典；
> ————————————————
> 原文链接：https://blog.csdn.net/weixin_43593330/article/details/108490956

```python
import torch
import torch.optim as optim


w1 = torch.randn(3, 3)
w1.requires_grad = True
w2 = torch.randn(3, 3)
w2.requires_grad = True
o = optim.Adam([w1])
print(o.param_groups)

outputs:
--------------------------------------------

[{'amsgrad': False,
  'betas': (0.9, 0.999),
  'eps': 1e-08,
  'lr': 0.001,
  'params': [tensor([[ 2.9064, -0.2141, -0.4037],
           [-0.5718,  1.0375, -0.6862],
           [-0.8372,  0.4380, -0.1572]])],
  'weight_decay': 0}]
```

```python
o.add_param_group({'params': w2})
print(o.param_groups)

outputs:
--------------------------------------------
[{'amsgrad': False,
  'betas': (0.9, 0.999),
  'eps': 1e-08,
  'lr': 0.001,
  'params': [tensor([[ 2.9064, -0.2141, -0.4037],
           [-0.5718,  1.0375, -0.6862],
           [-0.8372,  0.4380, -0.1572]])],
  'weight_decay': 0},
 {'amsgrad': False,
  'betas': (0.9, 0.999),
  'eps': 1e-08,
  'lr': 0.001,
  'params': [tensor([[-0.0560,  0.4585, -0.7589],
           [-0.1994,  0.4557,  0.5648],
           [-0.1280, -0.0333, -1.1886]])],
  'weight_decay': 0}]
```



